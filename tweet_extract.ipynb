{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-3.0.2-py3-none-any.whl (769 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\shamazha3\\anaconda3\\envs\\txt\\lib\\site-packages (from transformers) (20.4)\n",
      "Collecting sentencepiece!=0.1.92\n",
      "  Using cached sentencepiece-0.1.91-cp37-cp37m-win_amd64.whl (1.2 MB)\n",
      "Processing c:\\users\\shamazha3\\appdata\\local\\pip\\cache\\wheels\\69\\09\\d1\\bf058f7d6fa0ecba2ce7c66be3b8d012beb4bf61a6e0c101c0\\sacremoses-0.0.43-py3-none-any.whl\n",
      "Collecting requests\n",
      "  Using cached requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\shamazha3\\anaconda3\\envs\\txt\\lib\\site-packages (from transformers) (1.19.0)\n",
      "Collecting tokenizers==0.8.1.rc1\n",
      "  Using cached tokenizers-0.8.1rc1-cp37-cp37m-win_amd64.whl (1.9 MB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2020.7.14-cp37-cp37m-win_amd64.whl (268 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.48.0-py2.py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: six in c:\\users\\shamazha3\\anaconda3\\envs\\txt\\lib\\site-packages (from packaging->transformers) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\shamazha3\\anaconda3\\envs\\txt\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Collecting click\n",
      "  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-0.16.0-py3-none-any.whl (300 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shamazha3\\anaconda3\\envs\\txt\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Using cached urllib3-1.25.9-py2.py3-none-any.whl (126 kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Installing collected packages: sentencepiece, click, joblib, regex, tqdm, sacremoses, urllib3, chardet, idna, requests, tokenizers, filelock, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.8.0\n",
      "    Uninstalling tokenizers-0.8.0:\n",
      "      Successfully uninstalled tokenizers-0.8.0\n",
      "Successfully installed chardet-3.0.4 click-7.1.2 filelock-3.0.12 idna-2.10 joblib-0.16.0 regex-2020.7.14 requests-2.24.0 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 tqdm-4.48.0 transformers-3.0.2 urllib3-1.25.9\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: tensorflow in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: gast==0.3.3 in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow) (1.30.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.12.0 in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard<2.3.0,>=2.2.0 in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow) (0.9.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow) (1.19.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.3.0,>=2.2.0 in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in c:\\users\\shamazha3\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow) (3.2.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow) (3.12.2)\n",
      "Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (47.3.1.post20200622)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.18.0)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.6)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in c:\\users\\shamazha3\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.2'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformersNote: you may need to restart the kernel to use updated packages.\n",
      "  Cloning https://github.com/huggingface/transformers to c:\\users\\shamaz~1\\appdata\\local\\temp\\pip-req-build-pzgl3dv3\n",
      "Requirement already satisfied (use --upgrade to upgrade): transformers==3.0.2 from git+https://github.com/huggingface/transformers in c:\\users\\shamazha3\\anaconda3\\envs\\txt\\lib\\site-packages\n",
      "Requirement already satisfied: numpy in c:\\users\\shamazha3\\anaconda3\\envs\\txt\\lib\\site-packages (from transformers==3.0.2) (1.19.0)\n",
      "Collecting tokenizers==0.8.1.rc2\n",
      "  Downloading tokenizers-0.8.1rc2-cp37-cp37m-win_amd64.whl (1.9 MB)\n",
      "Requirement already satisfied: packaging in c:\\users\\shamazha3\\anaconda3\\envs\\txt\\lib\\site-packages (from transformers==3.0.2) (20.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\shamazha3\\anaconda3\\envs\\txt\\lib\\site-packages (from transformers==3.0.2) (3.0.12)\n",
      "Requirement already satisfied: requests in c:\\users\\shamazha3\\anaconda3\\envs\\txt\\lib\\site-packages (from transformers==3.0.2) (2.24.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\shamazha3\\anaconda3\\envs\\txt\\lib\\site-packages (from transformers==3.0.2) (4.48.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shamazha3\\anaconda3\\envs\\txt\\lib\\site-packages (from transformers==3.0.2) (2020.7.14)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in c:\\users\\shamazha3\\anaconda3\\envs\\txt\\lib\\site-packages (from transformers==3.0.2) (0.1.91)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\shamazha3\\anaconda3\\envs\\txt\\lib\\site-packages (from transformers==3.0.2) (0.0.43)\n",
      "Requirement already satisfied: six in c:\\users\\shamazha3\\anaconda3\\envs\\txt\\lib\\site-packages (from packaging->transformers==3.0.2) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\shamazha3\\anaconda3\\envs\\txt\\lib\\site-packages (from packaging->transformers==3.0.2) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\shamazha3\\anaconda3\\envs\\txt\\lib\\site-packages (from requests->transformers==3.0.2) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\shamazha3\\anaconda3\\envs\\txt\\lib\\site-packages (from requests->transformers==3.0.2) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\shamazha3\\anaconda3\\envs\\txt\\lib\\site-packages (from requests->transformers==3.0.2) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shamazha3\\anaconda3\\envs\\txt\\lib\\site-packages (from requests->transformers==3.0.2) (2020.6.20)\n",
      "Requirement already satisfied: click in c:\\users\\shamazha3\\anaconda3\\envs\\txt\\lib\\site-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\shamazha3\\anaconda3\\envs\\txt\\lib\\site-packages (from sacremoses->transformers==3.0.2) (0.16.0)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (setup.py): started\n",
      "  Building wheel for transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for transformers: filename=transformers-3.0.2-py3-none-any.whl size=798088 sha256=bda735121163be5c6c143d209eb63ada0a607acc1907eb9867a493b3f388ef2d\n",
      "  Stored in directory: C:\\Users\\SHAMAZ~1\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-7rub2a2p\\wheels\\35\\2e\\a7\\d819e3310040329f0f47e57c9e3e7a7338aa5e74c49acfe522\n",
      "Successfully built transformers\n",
      "Installing collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.8.1rc1\n",
      "    Uninstalling tokenizers-0.8.1rc1:\n",
      "      Successfully uninstalled tokenizers-0.8.1rc1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/huggingface/transformers 'C:\\Users\\SHAMAZ~1\\AppData\\Local\\Temp\\pip-req-build-pzgl3dv3'\n",
      "ERROR: transformers 3.0.2 has requirement tokenizers==0.8.1.rc1, but you'll have tokenizers 0.8.1rc2 which is incompatible.\n",
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\\\users\\\\shamazha3\\\\anaconda3\\\\envs\\\\txt\\\\lib\\\\site-packages\\\\~okenizers\\\\tokenizers.cp37-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import BertTokenizer\n",
    "import tokenizer\n",
    "import string\n",
    "import torch\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import tokenizers\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.autonotebook import tqdm\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "\n",
    "def jaccard(str1,str2):\n",
    "    a=set(str1.lower().split())\n",
    "    b=set(str2.lower().split())\n",
    "    c=a.intersection(b)\n",
    "    return float(len(c))/(len(a)+len(b)-len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(r'C:\\Users\\shamazha3\\Desktop\\Text\\tweet-sentiment-extraction\\train.csv')\n",
    "test=pd.read_csv(r'C:\\Users\\shamazha3\\Desktop\\Text\\tweet-sentiment-extraction\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "VALID_BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "BERT_PATH = r\"C:\\Users\\shamazha3\\Desktop\\Text\\Bert files\" \n",
    "MODEL_PATH = r\"C:\\Users\\shamazha3\\Desktop\\Text\\Bert files\\bert-base-uncased-pytorch_model.bin\"\n",
    "TRAINING_FILE = train\n",
    "TOKENIZER = tokenizers.BertWordPieceTokenizer(\n",
    "    os.path.join(BERT_PATH,\"vocab.txt\"), \n",
    "    lowercase=True\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': tensor([ 101, 8699,  102, 1045, 1036, 1040, 2031, 5838, 1010, 2065, 1045, 2020,\n",
      "        2183,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'token_type_ids': tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'tweet_tokens': '[CLS] neutral [SEP] i ` d have responded , if i were going [SEP]', 'targets': tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'targets_start': tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'targets_end': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'padding_len': tensor(114), 'orig_tweet': ' I`d have responded, if I were going', 'orig_selected': 'I`d have responded, if I were going', 'sentiment': tensor([1., 0., 0.]), 'orig_sentiment': 'neutral'}\n"
     ]
    }
   ],
   "source": [
    "class TweetDataset:\n",
    "    def __init__(self, tweet, sentiment, selected_text):\n",
    "        self.tweet = tweet\n",
    "        self.sentiment = sentiment\n",
    "        self.selected_text = selected_text\n",
    "        self.tokenizer = TOKENIZER\n",
    "        self.max_len = MAX_LEN\n",
    "    \n",
    "    def __len__(self): #length fo the dataset\n",
    "        return len(self.tweet)\n",
    "    \n",
    "    def __getitem__(self, item):  #get training data and preprocessing\n",
    "        tweet = \" \".join(str(self.tweet[item]).split())\n",
    "        selected_text = \" \".join(str(self.selected_text[item]).split())\n",
    "        \n",
    "        len_st = len(selected_text)\n",
    "        idx0 = -1\n",
    "        idx1 = -1\n",
    "        for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]): #match the index of characters in text and selected text\n",
    "            if tweet[ind: ind+len_st] == selected_text:\n",
    "                idx0 = ind  #start ind\n",
    "                idx1 = ind + len_st - 1  #end indz\n",
    "                break\n",
    "        #find character indeces\n",
    "        char_targets = [0] * len(tweet)   #Create a vector with zero values\n",
    "        #[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "        if idx0 != -1 and idx1 != -1:\n",
    "            for j in range(idx0, idx1 + 1):\n",
    "                if tweet[j] != \" \":\n",
    "                    char_targets[j] = 1\n",
    "        #[0,0,0,0,1,1,1,0,1,1,1,1,0,0,0]\n",
    "        \n",
    "        tok_tweet = self.tokenizer.encode(sequence=self.sentiment[item], pair=tweet)\n",
    "        tok_tweet_tokens = tok_tweet.tokens\n",
    "        tok_tweet_ids = tok_tweet.ids\n",
    "        tok_tweet_offsets = tok_tweet.offsets[3:-1]\n",
    "        # print(tok_tweet_tokens)\n",
    "        # print(tok_tweet.offsets)\n",
    "        # ['[CLS]', 'spent', 'the', 'entire', 'morning', 'in', 'a', 'meeting', 'w', '/', \n",
    "        # 'a', 'vendor', ',', 'and', 'my', 'boss', 'was', 'not', 'happy', 'w', '/', 'them', \n",
    "        # '.', 'lots', 'of', 'fun', '.', 'i', 'had', 'other', 'plans', 'for', 'my', 'morning', '[SEP]']\n",
    "        \n",
    "        #Creating target variable\n",
    "        targets = [0] * (len(tok_tweet_tokens) - 4)\n",
    "        if self.sentiment[item] == \"positive\" or self.sentiment[item] == \"negative\":\n",
    "            sub_minus = 8\n",
    "        else:\n",
    "            sub_minus = 7\n",
    "        \n",
    "        #[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]        \n",
    "        for j, (offset1, offset2) in enumerate(tok_tweet_offsets):\n",
    "            if sum(char_targets[offset1 - sub_minus:offset2 - sub_minus]) > 0: #checking for a partial match to label as 1\n",
    "                targets[j] = 1\n",
    "        #[0,0,0,0,1,1,1,0,1,1,1,1,0,0,0]        \n",
    "        \n",
    "        targets = [0] + [0] + [0] + targets + [0]  #cls, target and sep\n",
    "        targets_start = [0] * len(targets)\n",
    "        targets_end = [0] * len(targets)\n",
    "\n",
    "        non_zero = np.nonzero(targets)[0]\n",
    "        if len(non_zero) > 0:\n",
    "            targets_start[non_zero[0]] = 1\n",
    "            targets_end[non_zero[-1]] = 1\n",
    "        \n",
    "        #print(targets_start)\n",
    "        #print(targets_end)\n",
    "\n",
    "        # Mask\n",
    "        mask = [1] * len(tok_tweet_ids)\n",
    "        token_type_ids = [0] * 3 + [1] * (len(tok_tweet_ids) - 3)\n",
    "\n",
    "        #Padding\n",
    "        padding_length = self.max_len - len(tok_tweet_ids)\n",
    "        ids = tok_tweet_ids + ([0] * padding_length)\n",
    "        mask = mask + ([0] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        targets = targets + ([0] * padding_length)\n",
    "        targets_start = targets_start + ([0] * padding_length)\n",
    "        targets_end = targets_end + ([0] * padding_length)\n",
    "\n",
    "        sentiment = [1, 0, 0]\n",
    "        if self.sentiment[item] == \"positive\":\n",
    "            sentiment = [0, 0, 1]\n",
    "        if self.sentiment[item] == \"negative\":\n",
    "            sentiment = [0, 1, 0]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'tweet_tokens': \" \".join(tok_tweet_tokens),\n",
    "            'targets': torch.tensor(targets, dtype=torch.long),\n",
    "            'targets_start': torch.tensor(targets_start, dtype=torch.long),\n",
    "            'targets_end': torch.tensor(targets_end, dtype=torch.long),\n",
    "            'padding_len': torch.tensor(padding_length, dtype=torch.long),\n",
    "            'orig_tweet': self.tweet[item],\n",
    "            'orig_selected': self.selected_text[item],\n",
    "            'sentiment': torch.tensor(sentiment, dtype=torch.float),\n",
    "            'orig_sentiment': self.sentiment[item]\n",
    "        }\n",
    "    \n",
    "\n",
    "dset = TweetDataset( tweet=train.text.values, sentiment=train.sentiment.values, selected_text=train.selected_text.values )\n",
    "print(dset[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTBaseUncased(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTBaseUncased, self).__init__()\n",
    "        self.bert = transformers.BertModel.from_pretrained(r'C:\\Users\\shamazha3\\Desktop\\Text\\Bert files\\bert_config.json')\n",
    "        self.l0 = nn.Linear(768, 2) # Linear layer\n",
    "#r'C:\\Users\\shamazha3\\Desktop\\Text\\Bert files\\model.bin'   \n",
    "#r'C:\\Users\\shamazha3\\Desktop\\Text\\Bert files\\bert_config.json'\n",
    "    def forward(self, ids, mask, token_type_ids, sentiment):\n",
    "        \n",
    "        \n",
    "        o1, o2 = self.bert(\n",
    "            ids, \n",
    "            attention_mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        output= self.bert(\n",
    "            ids, \n",
    "            attention_mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        sequence_output = output[0]\n",
    "        pooled_output = output[1]\n",
    "        \n",
    "        # (batch_size, num_tokens, 768) #sequence output\n",
    "        logits = self.l0(sequence_output)\n",
    "        # (batch_size, num_tokens, 2) split into two \n",
    "        # (batch_size, num_tokens, 1), (batch_size, num_tokens, 1)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1) \n",
    "        start_logits = start_logits.squeeze(-1)  #where selected text is beginning\n",
    "        end_logits = end_logits.squeeze(-1)  #where selected text is ending\n",
    "        # (batch_size, num_tokens), (batch_size, num_tokens)\n",
    "        \n",
    "        start_logits = self.out1(torch.cat(start_logits, sentiment) , 1)\n",
    "        start_logits = self.out2(torch.cat(end_logits, sentiment) , 1)\n",
    "\n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(o1,o2,t1,t2):              #output1,2 & target 1,2\n",
    "    l1=nn.BCEWithLogitsLoss()(o1,t1)\n",
    "    l2=nn.BCEWithLogitsLoss()(o2,t2)\n",
    "    return l1+l2\n",
    "\n",
    "def train_fun(data_loader, model, optimizer, device, scheduler):\n",
    "    model.train()\n",
    "    losses =AverageMeter()\n",
    "    tk0=tqdm(data_loader, total=len(data_loader))\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for bi, d in tqdm(enumerate(tk0), total=len(data_loader)):\n",
    "            ids = d[\"ids\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            tweet_tokens = d[\"tweet_tokens\"]\n",
    "            target_start = d[\"target_start\"]\n",
    "            target_end = d[\"target_end\"]\n",
    "            \n",
    "            target_start = target_start.to(device, dtype=torch.float)\n",
    "            target_end = target_end.to(device, dtype=torch.float)            \n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            sentiment = sentiment.to(device, dtype=torch.float)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            o1, o2 = model(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            loss=loss_fn(o1,o2, targets_start, targets_end)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            losses.update(loss.item(),ids.size(0))\n",
    "            tk0.set_postfix(loss=losses.avg)\n",
    "            \n",
    "final_output = \"\"            \n",
    "def eval_fun(data_loader, model, device):\n",
    "    model.eval()\n",
    "    fin_output_start = []\n",
    "    fin_output_end = []\n",
    "    fin_padding_lens = []\n",
    "    fin_tweet_tokens = []\n",
    "    fin_orig_sentiment = []\n",
    "    fin_orig_selected = []\n",
    "    fin_orig_tweet = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            ids = d[\"ids\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            tweet_tokens = d[\"tweet_tokens\"]\n",
    "            padding_len = d[\"padding_len\"]\n",
    "            sentiment = d[\"sentiment\"]\n",
    "            orig_selected = d[\"orig_selected\"]\n",
    "            orig_sentiment = d[\"orig_sentiment\"]\n",
    "            orig_tweet = d[\"orig_tweet\"]\n",
    "            \n",
    "            \n",
    "                       \n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            sentiment = sentiment.to(device, dtype=torch.float)\n",
    "\n",
    "            \n",
    "            o1, o2 = model(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            \n",
    "            fin_output_start.append(torch.sigmoid(o1).cpu().detach().numpy())\n",
    "            fin_output_end.append(torch.sigmoid(o1).cpu().detach().numpy())\n",
    "            fin_padding_lens.extend(padding_len.cpu().detach().numpy())\n",
    "            \n",
    "            fin_tweet_tokens.extend(tweet_tokens)\n",
    "            fin_orig_sentiment.extend(orig_sentiment)\n",
    "            fin_orig_selected.extend(orig_selected)\n",
    "            fin_orig_tweet.extend(orig_tweet)\n",
    "            \n",
    "        fin_output_start = np.vstack(fin_output_start)\n",
    "        fin_output_end = np.vstack(fin_output_end) #create a numpy array\n",
    "        \n",
    "        #we will store some values\n",
    "        threshhold = 0.2        #>0.2 is our candidate\n",
    "        jaccards = []\n",
    "        for j in range(len(fin_tweet_tokens)): #loop in prdiction\n",
    "            target_string = fin_orig_selected[j]\n",
    "            tweet_tokens = fin_tweet_tokens[j]\n",
    "            padding_len = fin_padding_len[j]\n",
    "            original_tweet = fin_orig_tweet[j]\n",
    "            sentiment = fin_orig_sentiment[j]\n",
    "            \n",
    "            if padding_len > 0:\n",
    "                mask_start = fin_output_start[j, :][:-padding_len] >= threshhold\n",
    "                mask_end = fin_output_end[j, :][:-padding_len] >= threshhold\n",
    "            else:    \n",
    "                mask_start = fin_output_start[j, :] >= threshhold\n",
    "                mask_end = fin_output_end[j, :] >= threshhold\n",
    "                \n",
    "            mask = [0] * len(mask_start)\n",
    "            idx_start = np.nonzero(mask_start)[0]\n",
    "            idx_end = np.nonzero(mask_end)[0]\n",
    "            \n",
    "            if len(idx_start) > 0:\n",
    "                idx_start = idx_start[0]\n",
    "                if len(idx_end) > 0:\n",
    "                    idx_end = idx_end[0]\n",
    "                else:\n",
    "                    idx_start = 0\n",
    "                    idx_end = 0\n",
    "                    \n",
    "            for mj in range(idx_Start, idx_end + 1):\n",
    "                mask[mj] = 1\n",
    "                \n",
    "            output_tokens = [x for p, x in enumerate(tweet_tokens.split()) if mask[p] == 1]\n",
    "            output_tokens = [x for x in output_tokens if x not in (\"[CLS]\", \"[SEP]\")]\n",
    "            \n",
    "#             final_output = \"\"\n",
    "            for ot in output_tokens:\n",
    "                if ot.startswith(\"##\"):\n",
    "                    final_output = final_output + ot[2:]\n",
    "                elif len(ot) ==1 and ot in string.punctuation:\n",
    "                    final_output = final_output + ot\n",
    "                else:\n",
    "                     final_output = final_output + \" \" + ot\n",
    "            final_output = final_output.strip()\n",
    "            print(final_output)\n",
    "            # if sentiment is neutral\n",
    "            if sentiment  == \"neutral\" or len(original_tweet.split()) < 4:\n",
    "                final_output = original_tweet\n",
    "            \n",
    "            jac = utils.jaccard(target_string.strip(), final_output.strip())\n",
    "            jaccards.append(jac)\n",
    "            \n",
    "        mean_jac = np.mean(jaccards)\n",
    "    return mean_jac\n",
    "            \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mod():\n",
    "    dfx= pd.read_csv(r'C:\\Users\\shamazha3\\Desktop\\Text\\tweet-sentiment-extraction\\train.csv', nrows=1000).dropna().reset_index(drop=True)\n",
    "    \n",
    " \n",
    "    df_train, df_valid = model_selection.train_test_split(\n",
    "        dfx, test_size=0.1, random_state=42, stratify=dfx.sentiment.values\n",
    "    )\n",
    "\n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_valid = df_valid.reset_index(drop=True)\n",
    "\n",
    "    train_dataset = TweetDataset(\n",
    "        tweet=df_train.text.values, sentiment=df_train.sentiment.values,selected_text=df_train.selected_text.values\n",
    "    )\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=4\n",
    "    )\n",
    "\n",
    "    valid_dataset = TweetDataset(\n",
    "        tweet=df_valid.text.values, sentiment=df_valid.sentiment.values,selected_text=df_valid.selected_text.values\n",
    "    )\n",
    "\n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=VALID_BATCH_SIZE, num_workers=1\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "    model = BERTBaseUncased()\n",
    "    model.to(device)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.001,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    num_train_steps = int(len(df_train) / TRAIN_BATCH_SIZE * EPOCHS)\n",
    "    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n",
    "    )\n",
    "\n",
    "#     model = nn.DataParallel(model)\n",
    "\n",
    "    best_jaccard = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        engine.train_fn(train_data_loader, model, optimizer, device, scheduler)\n",
    "        jaccard = engine.eval_fn(valid_data_loader, model, device)\n",
    "        outputs = np.array(outputs) >= 0.5\n",
    "        \n",
    "        print(f\"jaccard score= {jaccard}\")\n",
    "        if accuracy > best_jaccard:\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            best_accuracy = accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to load weights from pytorch checkpoint file. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    675\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m                 \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    592\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    594\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 763\u001b[1;33m     \u001b[0mmagic_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    764\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmagic_number\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mMAGIC_NUMBER\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: invalid load key, '{'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-a259961f2964>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-53-a085fb74d58a>\u001b[0m in \u001b[0;36mmod\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBERTBaseUncased\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-52-eadd48128dab>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBERTBaseUncased\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBertModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'C:\\Users\\shamazha3\\Desktop\\Text\\Bert files\\bert_config.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m768\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Linear layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#r'C:\\Users\\shamazha3\\Desktop\\Text\\Bert files\\model.bin'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    676\u001b[0m                 \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m                 raise OSError(\n\u001b[0m\u001b[0;32m    679\u001b[0m                     \u001b[1;34m\"Unable to load weights from pytorch checkpoint file. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m                     \u001b[1;34m\"If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to load weights from pytorch checkpoint file. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. "
     ]
    }
   ],
   "source": [
    "mod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01082688c6</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33987a8ee5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID selected_text\n",
       "0  f87dea47db              \n",
       "1  96d74cb729              \n",
       "2  eee518ae67              \n",
       "3  01082688c6              \n",
       "4  33987a8ee5              "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def post_process(selected):\n",
    "    return \" \".join(set(selected.lower().split()))\n",
    "\n",
    "result = pd.read_csv(r\"C:\\Users\\shamazha3\\Desktop\\Text\\tweet-sentiment-extraction\\sample_submission.csv\")\n",
    "result.loc[:, 'selected_text'] = final_output\n",
    "result.selected_text = result.selected_text.map(post_process)\n",
    "result.to_csv(r\"C:\\Users\\shamazha3\\Desktop\\Text\\tweet-sentiment-extraction\\sample_submission.csv\", index=False)\n",
    "result.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
